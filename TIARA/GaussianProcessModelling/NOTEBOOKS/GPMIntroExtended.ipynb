{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================================================================\n",
    "\n",
    "&lsaquo; GPMIntro.ipynb  &rsaquo;\n",
    "Copyright (C) &lsaquo; 2017 &rsaquo;  &lsaquo; Anna Scaife - anna.scaife@manchester.ac.uk &rsaquo;\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\n",
    "==============================================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[AMS - 170402]  Notebook created for SKA-SA Newton Big Data Summer School, Cape Town , April 2017.\n",
    "--\n",
    "\n",
    "This notebook is an extended version of the notebook \"GPMIntro.ipynb\". This notebook uses GPM to predict a signal.  It recreates some of the plots from Roberts et al. 2013 (http://www.robots.ox.ac.uk/~sjrob/Pubs/Phil.%20Trans.%20R.%20Soc.%20A-2013-Roberts-.pdf). \n",
    "\n",
    "It extends the original notebook to show how to fit maximum likelihood hyper-parameters to covariance kernels using a combination of the **george** library (http://dan.iel.fm/george/current/) and the **emcee** library (http://dan.iel.fm/emcee/current/).\n",
    "\n",
    "It is a teaching resource and is accompanied by the lecture \"Can you Predict the Future..?\".\n",
    "\n",
    "All Python libraries used in this example can be installed using **pip**.\n",
    "\n",
    "---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start let's specify that we want our figures to appear embedded in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's import all the libraries we need..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "from scipy import linalg as sl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the covariance kernel a squared-exponential,\n",
    "\n",
    "$k(x_1,x_2) = h^2 \\exp{ \\left( \\frac{-(x_1 - x_2)^2}{\\lambda^2} \\right)}$,\n",
    "\n",
    "just like Eq. 3.11 in Roberts et al. (2012).\n",
    "\n",
    "http://www.robots.ox.ac.uk/~sjrob/Pubs/Phil.%20Trans.%20R.%20Soc.%20A-2013-Roberts-.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cov_kernel(x1,x2,h,lam):\n",
    "    \"\"\"\n",
    "    Squared-Exponential covariance kernel\n",
    "    \"\"\"\n",
    "    k12 = h**2*np.exp(-1.*(x1 - x2)**2/lam**2)\n",
    "    \n",
    "    return k12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this kernel to calculate  the value of each element in our covariance matrix:\n",
    "\n",
    "$\\mathbf{K(x,x)} = \\left(\n",
    "\\begin{array}{cccc}\n",
    "k(x_1,x_1) & k(x_1,x_2) & ... & k(x_1,x_n) \\\\\n",
    "k(x_2,x_1) & k(x_2,x_2) & ... & k(x_2,x_n) \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "k(x_n,x_1) & k(x_n,x_2) & ... & k(x_n,x_n) \n",
    "\\end{array}\n",
    "\\right).$\n",
    "\n",
    "We can then populate a covariance matrix, $K(\\mathbf{x},\\mathbf{x})$, for our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_K(x, h, lam):\n",
    "    \n",
    "    \"\"\"\n",
    "    Make covariance matrix from covariance kernel\n",
    "    \"\"\"\n",
    "    \n",
    "    # for a data array of length x, make a covariance matrix x*x:\n",
    "    K = np.zeros((len(x),len(x)))\n",
    "    \n",
    "    for i in range(0,len(x)):\n",
    "        for j in range(0,len(x)):\n",
    "            \n",
    "            if (i==j):\n",
    "                noise = 1e-5\n",
    "            else:\n",
    "                noise = 0.0\n",
    "            \n",
    "            # calculate value of K for each separation:\n",
    "            K[i,j] = cov_kernel(x[i],x[j],h,lam) + noise**2\n",
    "            \n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this kernel we can then recreate Fig. 5 from Roberts et al. (2012)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an array of 200 evenly spaced positions between 0 and 20:\n",
    "x1 = np.arange(0, 20.,0.01)\n",
    "    \n",
    "for i in range(0,3):\n",
    "    \n",
    "    h = 1.0\n",
    "    \n",
    "    if (i==0): lam = 0.1\n",
    "    if (i==1): lam = 1.0\n",
    "    if (i==2): lam = 5.0\n",
    "        \n",
    "    # make a covariance matrix:\n",
    "    K = make_K(x1,h,lam)\n",
    "    \n",
    "    # five realisations:\n",
    "    for j in range(0,5):\n",
    "        \n",
    "        # draw samples from a co-variate Gaussian distribution, N(0,K):\n",
    "        y1 = np.random.multivariate_normal(np.zeros(len(x1)),K)\n",
    "    \n",
    "        tmp2 = '23'+str(i+3+1)\n",
    "        pl.subplot(int(tmp2))\n",
    "        pl.plot(x1,y1)\n",
    "        \n",
    "        \n",
    "    tmp1 = '23'+str(i+1)\n",
    "    pl.subplot(int(tmp1))\n",
    "    pl.imshow(K)\n",
    "    pl.title(r\"$\\lambda = $\"+str(lam))\n",
    "    \n",
    "    \n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then take the final realization, which has $\\lambda = 5$, and select 5 points from it randomly we can calculate the posterior mean and variance at every point based on those five input data. \n",
    "\n",
    "The mean and variance are given by Eq. 3.8 & 3.9 in Roberts et al. (2012) or Eq. 2.25 & 2.26 in Rasmussen & Williams.\n",
    "\n",
    "First let's select our  **training data points** and our **test data points**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set number of training points\n",
    "nx_training = 5\n",
    "\n",
    "# randomly select the training points:\n",
    "tmp = np.random.uniform(low=0.0, high=2000.0, size=nx_training)\n",
    "tmp = tmp.astype(int)\n",
    "\n",
    "condition = np.zeros_like(x1)\n",
    "for i in tmp: condition[i] = 1.0\n",
    "    \n",
    "y_train = y1[np.where(condition==1.0)]\n",
    "x_train = x1[np.where(condition==1.0)]\n",
    "y_test = y1[np.where(condition==0.0)]\n",
    "x_test = x1[np.where(condition==0.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use our **training data points** to define a covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "determinant:  0.00115367123152 0.00115367123152\n"
     ]
    }
   ],
   "source": [
    "# define the covariance matrix:\n",
    "K = make_K(x_train,h,lam)\n",
    "\n",
    "# take the inverse:\n",
    "iK = np.linalg.inv(K)\n",
    "\n",
    "print 'determinant: ',np.linalg.det(K), sl.det(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of our **test data points** we can then make a prediction of the value at $x_{\\ast}$ and the uncertainly (standard deviation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu=[];sig=[]\n",
    "for xx in x_test:\n",
    "\n",
    "    # find the 1d covariance matrix:\n",
    "    K_x = cov_kernel(xx, x_train, h, lam)\n",
    "    \n",
    "    # find the kernel for (x,x):\n",
    "    k_xx = cov_kernel(xx, xx, h, lam)\n",
    "    \n",
    "    # calculate the posterior mean and variance:\n",
    "    mu_xx = np.dot(K_x.T,np.dot(iK,y_train))\n",
    "    sig_xx = k_xx - np.dot(K_x.T,np.dot(iK,K_x))\n",
    "    \n",
    "    mu.append(mu_xx)\n",
    "    sig.append(np.sqrt(np.abs(sig_xx))) # note sqrt to get stdev from variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu and sig are currently lists - turn them into numpy arrays:\n",
    "mu=np.array(mu);sig=np.array(sig)\n",
    "\n",
    "# make some plots:\n",
    "\n",
    "# left-hand plot\n",
    "ax = pl.subplot(121)\n",
    "pl.scatter(x_train,y_train)  # plot the training points\n",
    "pl.plot(x1,y1,ls=':')        # plot the original data they were drawn from\n",
    "pl.title(\"Input\")\n",
    "\n",
    "# right-hand plot\n",
    "ax = pl.subplot(122)\n",
    "pl.plot(x_test,mu,ls='-')     # plot the predicted values\n",
    "pl.plot(x_test,y_test,ls=':') # plot the original values\n",
    "\n",
    "\n",
    "# shade in the area inside a one standard deviation bound:\n",
    "ax.fill_between(x_test,mu-sig,mu+sig,facecolor='lightgrey', lw=0, interpolate=True)\n",
    "pl.title(\"Predicted\")\n",
    "\n",
    "pl.scatter(x_train,y_train)  # plot the training points\n",
    "\n",
    "# display the plot:\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Note: Depending on  the selection of training points you might want to specify some axis ranges for these plots]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we've come to the end of the material presented in the lecture.\n",
    "\n",
    "--END--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra material\n",
    "--\n",
    "\n",
    "What if we don't know the values of the hyper-parameters in our covariance kernel? We can fit them directly from the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of training points\n",
    "frac = 0.05\n",
    "nx_training = int(len(x1)*frac)\n",
    "print \"Using \",nx_training,\" points.\"\n",
    "\n",
    "# randomly select the training points:\n",
    "tmp = np.random.uniform(low=0.0, high=2000.0, size=nx_training)\n",
    "tmp = tmp.astype(int)\n",
    "\n",
    "condition = np.zeros_like(x1)\n",
    "for i in tmp: condition[i] = 1.0\n",
    "    \n",
    "y_train = y1[np.where(condition==1.0)]\n",
    "x_train = x1[np.where(condition==1.0)]\n",
    "y_test = y1[np.where(condition==0.0)]\n",
    "x_test = x1[np.where(condition==0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.scatter(x_train,y_train)  # plot the training points\n",
    "pl.plot(x1,y1,ls=':')        # plot the original data they were drawn from\n",
    "pl.title(\"Training Data\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For what follows I'm going to use the **george** library. The main reason for this is that the matrix inversion in **numpy** is not very stable for large (> (10 x 10)) matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import george\n",
    "from george import kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "# we need to define three functions: \n",
    "# a log likelihood, a log prior & a log posterior.\n",
    "\n",
    "# set the loglikelihood:\n",
    "def lnlike2(p, x, y):\n",
    "    \n",
    "    # update kernel parameters:\n",
    "    gp.kernel[:] = p\n",
    "    \n",
    "    # compute covariance matrix:\n",
    "    gp.compute(x)\n",
    "    \n",
    "    # calculate the likelihood:\n",
    "    ll = gp.lnlikelihood(y, quiet=True)\n",
    "    \n",
    "    # return \n",
    "    return ll if np.isfinite(ll) else 1e25\n",
    "\n",
    "# set the logprior\n",
    "def lnprior2(p):\n",
    "    \n",
    "    # note that \"p\" contains the ln()\n",
    "    # of the parameter values - set your\n",
    "    # prior ranges appropriately!\n",
    "    \n",
    "    lnh,lnlam,lnsig = p\n",
    "    \n",
    "    # really crappy prior:\n",
    "    if (-2.<lnh<5.) and (-1.<lnlam<10.) and (-30.<lnsig<0.):\n",
    "        return 0.0\n",
    "    \n",
    "    return -np.inf\n",
    "\n",
    "# set the logposterior:\n",
    "def lnprob2(p, x, y):\n",
    "    \n",
    "    lp = lnprior2(p)\n",
    "    \n",
    "    return lp + lnlike2(p, x, y) if np.isfinite(lp) else -np.inf\n",
    "\n",
    "\n",
    "# initiate george with the exponential squared kernel:\n",
    "kernel = 1.0*kernels.ExpSquaredKernel(30.0)+kernels.WhiteKernel(1e-5)\n",
    "gp = george.GP(kernel, mean=0.0)\n",
    "\n",
    "# put all the data into a single array:\n",
    "data = (x_train,y_train)\n",
    "\n",
    "# set your initial guess parameters\n",
    "# remember george keeps these in ln() form!\n",
    "initial = gp.kernel[:]\n",
    "print \"Initial guesses: \",np.exp(initial)\n",
    "\n",
    "# set the dimension of the prior volume \n",
    "# (i.e. how many parameters do you have?)\n",
    "ndim = len(initial)\n",
    "print \"Number of parameters: \",ndim\n",
    "\n",
    "# The number of walkers needs to be more than twice \n",
    "# the dimension of your parameter space... unless you're crazy!\n",
    "nwalkers = 10\n",
    "\n",
    "# perturb your inital guess parameters very slightly\n",
    "# to get your starting values:\n",
    "p0 = [np.array(initial) + 1e-4 * np.random.randn(ndim)\n",
    "      for i in xrange(nwalkers)]\n",
    "\n",
    "# initalise the sampler:\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob2, args=data)\n",
    "\n",
    "# run a few samples as a burn-in:\n",
    "print(\"Running burn-in\")\n",
    "p0, lnp, _ = sampler.run_mcmc(p0, 1000)\n",
    "sampler.reset()\n",
    "\n",
    "# take the highest likelihood point from the burn-in as a\n",
    "# starting point and now begin your production run:\n",
    "print(\"Running production\")\n",
    "p = p0[np.argmax(lnp)]\n",
    "p0 = [p + 1e-4 * np.random.randn(ndim) for i in xrange(nwalkers)]\n",
    "p0, _, _ = sampler.run_mcmc(p0, 8000)\n",
    "\n",
    "print \"Finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import acor\n",
    "\n",
    "# calculate the convergence time of our\n",
    "# MCMC chains:\n",
    "samples = sampler.flatchain\n",
    "s2 = np.ndarray.transpose(samples)\n",
    "tau, mean, sigma = acor.acor(s2)\n",
    "print \"Convergence time from acor: \", tau\n",
    "\n",
    "# get rid of the samples that were taken\n",
    "# before convergence:\n",
    "delta = int(20*tau)\n",
    "samples = sampler.flatchain[delta:,:]\n",
    "\n",
    "# extract the log likelihood for each sample:\n",
    "lnl = sampler.flatlnprobability[delta:]\n",
    "\n",
    "# find the point of maximum likelihood:\n",
    "ml = samples[np.argmax(lnl),:]\n",
    "\n",
    "# print out those values\n",
    "# Note that we have unwrapped \n",
    "print \"ML parameters: \", \n",
    "print \"h: \", np.sqrt(np.exp(ml[0])),\" ; lam: \",np.sqrt(np.exp(ml[1])),\" ; sigma: \",np.sqrt(np.exp(ml[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the george **ExpSquaredKernel** has a factor of two in the denominator of the exponent. So whereas we specified that  $\\lambda_{\\rm true} = 5.0$, we have found that $\\lambda_{\\rm fit} = 3.46$...  (or similar)\n",
    "\n",
    "Is this what we should expect given the extra factor of two? Well, 25/2 = 12.5 and $\\sqrt{12.5} =  \\dots 3.54$, so we're pretty close!\n",
    "\n",
    "Let's plot our probability surfaces for each pair of parameters, as well as the confidence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "# Plot it.\n",
    "figure = corner.corner(samples, labels=[r\"$\\ln(h^2)$\", r\"$\\ln(\\lambda^2)$\", r\"$\\ln(\\sigma^2)$\"],\n",
    "                         truths=ml,\n",
    "                         quantiles=[0.16,0.5,0.84],\n",
    "                         #levels=[0.39,0.86,0.99],\n",
    "                         levels=[0.68,0.95,0.99],\n",
    "                         title=\"Mauna Loa Data\",\n",
    "                         show_titles=True, title_args={\"fontsize\": 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  can also extract the **expectation** value for each parameter and the individual confidence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = corner.quantile(samples[:,0],\n",
    "\t\t\t\t\t\t q=[0.16,0.5,0.84])\n",
    "\n",
    "print \"Parameter 1: \",q1[1],\"(-\",q1[1]-q1[0],\", +\",q1[2]-q1[1],\")\"\n",
    "\n",
    "q2 = corner.quantile(samples[:,1],\n",
    "\t\t\t\t\t\t q=[0.16,0.5,0.84])\n",
    "\n",
    "print \"Parameter 2: \",q2[1],\"(-\",q2[1]-q2[0],\", +\",q2[2]-q2[1],\")\"\n",
    "\n",
    "q3 = corner.quantile(samples[:,2],\n",
    "\t\t\t\t\t\t q=[0.16,0.5,0.84])\n",
    "\n",
    "print \"Parameter 3: \",q2[1],\"(-\",q2[1]-q2[0],\", +\",q2[2]-q2[1],\")\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
